{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64804090",
   "metadata": {},
   "source": [
    "# Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744d85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment:\n",
    "    POSITIVE = \"POSITIVE\"\n",
    "    NEGATIVE = \"NEGATIVE\" \n",
    "\n",
    "class Data:\n",
    "    def __init__(self, tweet, replies, retweets, likes):\n",
    "        self.tweet = tweet\n",
    "        self.replies = replies\n",
    "        self.retweets = retweets\n",
    "        self.likes = likes\n",
    "        self.sentiment = self.get_sentiment()\n",
    "        \n",
    "    def get_sentiment(self):\n",
    "        if self.replies >= '200' or self.retweets >= '300' or self.likes >= '500':\n",
    "            return Sentiment.POSITIVE\n",
    "        else:\n",
    "            return Sentiment.NEGATIVE\n",
    "        \n",
    "class DataContainer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def get_text(self):\n",
    "        return [x.tweet for x in self.data]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.data]\n",
    "        \n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.data))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.data))\n",
    "        negative_shrunk = negative[:len(positive)]\n",
    "        self.data = positive + negative_shrunk\n",
    "        random.shuffle(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750540a4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e455ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: row 1512903 was not complete.\n",
      "Error: row 1574085 was not complete.\n",
      "Error: row 1668073 was not complete.\n",
      "Error: row 1728665 was not complete.\n",
      "Error: row 1790437 was not complete.\n",
      "Error: row 1817089 was not complete.\n",
      "Error: row 1875356 was not complete.\n",
      "Error: row 1932282 was not complete.\n",
      "Error: row 1967754 was not complete.\n",
      "NEGATIVE\n",
      "@monicaonairtalk We stopped at the mask. Before corona the Lord led me, showed me a plot to inoculate the world with a plant-based vaccine. My husband thought I was nuts, then the news of the “virus” came. Praise God he could see. We don’t know his status, but he hasn’t served since March 2020\n",
      "\n",
      "Data length: 1999991\n",
      "Faulty rows: 9\n",
      "Max replies: 6382\n",
      "Max retweets: 19778\n",
      "Max likes: 81874\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "i = 1\n",
    "faulty_rows = 0\n",
    "max_replies = 0\n",
    "max_retweets = 0\n",
    "max_likes = 0\n",
    "\n",
    "with open(r\"final_data.csv\", encoding = 'utf-8') as f:    \n",
    "    csvReader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for row in csvReader:\n",
    "        if (i > 2000000): #total tweets = 54243060 (faulty rows included)\n",
    "            break\n",
    "        if (len(row) == 4):\n",
    "            if(row[1].isnumeric() and row[2].isnumeric() and row[3].isnumeric()):\n",
    "                tweet = row[0]\n",
    "                replies_count = row[1]\n",
    "                retweets_count = row[2]\n",
    "                likes_count = row[3]\n",
    "                data.append(Data(tweet, replies_count, retweets_count, likes_count))\n",
    "                #Find max replies, retweets, likes\n",
    "                if (int(replies_count) > max_replies):\n",
    "                    max_replies = int(replies_count)\n",
    "                if (int(retweets_count) > max_retweets):\n",
    "                    max_retweets = int(retweets_count)    \n",
    "                if (int(likes_count) > max_likes):\n",
    "                    max_likes = int(likes_count)\n",
    "            else:\n",
    "                print(\"Error: row \" + str(i) + \" was incorrect.\")\n",
    "                faulty_rows = faulty_rows + 1\n",
    "        else:\n",
    "            print(\"Error: row \" + str(i) + \" was not complete.\")\n",
    "            faulty_rows = faulty_rows + 1\n",
    "        i = i + 1\n",
    "f.close()\n",
    "\n",
    "print(data[850].sentiment)\n",
    "print(data[850].tweet)\n",
    "print(f'\\nData length: ' + str(len(data)))\n",
    "print('Faulty rows: ' + str(faulty_rows))\n",
    "print('Max replies: ' + str(max_replies))\n",
    "print('Max retweets: ' + str(max_retweets))\n",
    "print('Max likes: ' + str(max_likes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8a71e",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6fcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lambr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lambr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monicaonairtalk we stopped at the mask before corona the lord led me showed me plot to inoculate the world with plant based vaccine my husband thought wa nut then the news of the virus came praise god he could see we don know his status but he hasn served since march 2020\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    # Remove all the special characters (punctuation removal)\n",
    "    document = re.sub(r'[^a-zA-Z0-9]', ' ', str(data[i].tweet))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    # Tokenization for stemming\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    data[i].tweet = document\n",
    "\n",
    "print(data[850].tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7508f2",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2363e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143525\n",
      "143525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_container = DataContainer(training)\n",
    "test_container = DataContainer(test)\n",
    "\n",
    "train_container.evenly_distribute()\n",
    "train_x = train_container.get_text()\n",
    "train_y = train_container.get_sentiment()\n",
    "\n",
    "test_container.evenly_distribute()\n",
    "test_x = test_container.get_text()\n",
    "test_y = test_container.get_sentiment()\n",
    "\n",
    "print(train_y.count(Sentiment.POSITIVE))\n",
    "print(train_y.count(Sentiment.NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7dc9cc",
   "metadata": {},
   "source": [
    "# Bag of words vectorization to TFIDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb3718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lambr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "august5th 1 in last 24 hour pakistan coronavirus case increased by 5661 nationally reaching 1 053 660 derived from 62 462 test death increased by 60 current covid19 critical case are 192 recovery ha been of 1454 covid thursdaythoughts breakingnews corona ncoc http co eofebfrmjn\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=20, max_df=0.3, stop_words=stopwords.words('english'))\n",
    "\n",
    "train_x_vectors = np.asarray(vectorizer.fit_transform(train_x).todense())\n",
    "test_x_vectors = np.asarray(vectorizer.transform(test_x).todense())\n",
    "\n",
    "print(test_x[0])\n",
    "print(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a17f1",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23968b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'sentiment_classifier_2000000_tweets.pkl', 'rb') as f:\n",
    "    classification_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-temperature",
   "metadata": {},
   "source": [
    "# Use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b69a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LinearSVC(C=1, max_iter=10000, random_state=42), DecisionTreeClassifier(random_state=42, splitter='random'), KNeighborsClassifier(n_jobs=-1, weights='distance'), GaussianNB(), LogisticRegression(n_jobs=-1, random_state=42, solver='sag'), RandomForestClassifier(max_features='log2', n_estimators=200, n_jobs=-1,\n",
      "                       random_state=42), MLPClassifier(max_iter=10, random_state=42, warm_start=True)]\n",
      "\n",
      "\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n",
      "['NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "test_set = ['Attention Covid is real! He affects our respiratory organs and system!']\n",
    "new_test = vectorizer.transform(test_set).toarray()\n",
    "\n",
    "print(classification_model)\n",
    "print()\n",
    "print()\n",
    "print(classification_model[0].predict(new_test))\n",
    "print(classification_model[1].predict(new_test))\n",
    "print(classification_model[2].predict(new_test))\n",
    "print(classification_model[3].predict(new_test))\n",
    "print(classification_model[4].predict(new_test))\n",
    "print(classification_model[5].predict(new_test))\n",
    "print(classification_model[6].predict(new_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722b862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
